{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b318748",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974af06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 get GC content from contigs\n",
    "    #get GC content in set chunk sizes\n",
    "    \n",
    "#2 get coverage for contigs\n",
    "    #get coverage in chunk sizes\n",
    "\n",
    "#3 blob plot gc vs average coverage\n",
    "\n",
    "#4 plot gc content/coverage for each contig (if significant in some way)\n",
    "\n",
    "#5 kmeans clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7684a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2893ee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ec1c42",
   "metadata": {},
   "source": [
    "assembly : the fasta input\n",
    "coverage_per_contig : A tsv file with information for each contig\n",
    "coverage_per_base:  A tsv file with coverage information for each base pair position\n",
    "out_dir : output directory\n",
    "min_contig_size : the minimum size of a contig, otherwise its ignored\n",
    "chunk_size : the chunk size for dividing up longer contigs (default 10000)\n",
    "visualize_GC : An option for creating plots showing the distribution of GC content accross a contig in steps of a given chunk size (default = False)\n",
    "save_GC_plot : option to create an output dir savinig a plot for each GC plot created by visualizeGC\n",
    "visualize_Coverage : An option to create plots showing coverage distribution per contig\n",
    "save_coverage_plot : option to save coverage plots in out dir\n",
    "threshold_pct : The percentage away from the mean that something needs to be in order to be highlighted\n",
    "visualize_blob : Option to visualize blob plot\n",
    "save_blob_plot : Option to save blob plot\n",
    "visualize_histogram : Option to cisualize histogram of conitg lengths\n",
    "histogram_bin_size: size of bins form historgam plotting contig lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1180c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = \"./data/final.p_ctg.fasta\"\n",
    "coverage_per_contig = \"./data/heliopora_coerulea_hifi_2_p_assembly.coverage\"\n",
    "coverage_per_base = \"./data/heliopora_coerulea_hifi_2_p_assembly.depth\"\n",
    "out_dir = \"out_dir/\"\n",
    "min_contig_size = 292158\n",
    "chunk_size = 10000\n",
    "visualize_GC = False\n",
    "save_GC_plot = False\n",
    "visualize_Coverage = False\n",
    "save_coverage_plot = False\n",
    "threshold_pct = 50\n",
    "visualize_blob = True\n",
    "save_blob_plot = True\n",
    "plot_min_length = 2031501\n",
    "visualize_histogram = True\n",
    "save_histogram_plot = True\n",
    "histogram_bin_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b434043",
   "metadata": {},
   "source": [
    "# Naive GC content for each content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff775cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content(fasta_file):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to their GC content\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq y min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:\n",
    "                        gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "                        \n",
    "                # Start the new contig\n",
    "                current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "    \n",
    "    # Calculate GC content for the final contig\n",
    "    gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "    \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c675f328",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/final.p_ctg.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m naive_gc \u001b[38;5;241m=\u001b[39m \u001b[43mget_gc_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43massembly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#print(naive_gc)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(naive_gc))\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mget_gc_content\u001b[1;34m(fasta_file)\u001b[0m\n\u001b[0;32m      6\u001b[0m current_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      7\u001b[0m current_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;66;03m# If this is a new contig, calculate the GC content for the previous one (if there was one)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Heliopora\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/final.p_ctg.fasta'"
     ]
    }
   ],
   "source": [
    "naive_gc = get_gc_content(assembly)\n",
    "#print(naive_gc)\n",
    "print(len(naive_gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad90d78",
   "metadata": {},
   "source": [
    "# GC content for each contig by averaging in given chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad03fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content_chunks(fasta_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to a list of gc content per chunk size\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq < min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:  \n",
    "                        i = 0\n",
    "                        while i < len(current_seq):\n",
    "                            # check for last chunk\n",
    "                            if (i + chunk_size) > len(current_seq):\n",
    "                                end = len(current_seq)\n",
    "                            else:\n",
    "                                end = i + chunk_size\n",
    "                            sub = current_seq[i:end]\n",
    "                            gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)),2)\n",
    "                            chunks.append(gc)\n",
    "                            i = end\n",
    "                        # add to dictionary in form {current_id : [array of GC content of given chunk size]}\n",
    "                        gc_content[current_id] = [chunks, len(current_seq)]\n",
    "\n",
    "                # Start the new contig\n",
    "                current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_seq = \"\"\n",
    "                chunks = []\n",
    "                \n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "                \n",
    "        # calculate GC content for the last contig\n",
    "        if current_id is not None:\n",
    "            i = 0\n",
    "            while i < len(current_seq):\n",
    "                # check for last chunk\n",
    "                if (i + chunk_size) > len(current_seq):\n",
    "                    end = len(current_seq)\n",
    "                else:\n",
    "                    end = i + chunk_size\n",
    "                sub = current_seq[i:end]\n",
    "                gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)), 2)\n",
    "                chunks.append(gc)\n",
    "                i = end\n",
    "            # add to dictionary in form {current_id : [array of GC content of given chunk size, length]}\n",
    "            gc_content[current_id] = [chunks, len(current_seq)]\n",
    "        \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d18eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/final.p_ctg.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mget_gc_content_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43massembly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m gc_trimmed_mean \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Get the trimmed mean from chunks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mget_gc_content_chunks\u001b[1;34m(fasta_file, chunk_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m current_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;66;03m# If this is a new contig, calculate the GC content for the previous one (if there was one)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Heliopora\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/final.p_ctg.fasta'"
     ]
    }
   ],
   "source": [
    "chunks = get_gc_content_chunks(assembly, chunk_size)\n",
    "gc_trimmed_mean = {}\n",
    "#Get the trimmed mean from chunks\n",
    "for entry in chunks:\n",
    "    length = chunks.get(entry)[1]\n",
    "    avg_cov = chunks.get(entry)[0]\n",
    "    gc_trimmed_mean[entry] = [stats.trim_mean(avg_cov, 0.05), length]\n",
    "#print(gc_trimmed_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2afd",
   "metadata": {},
   "source": [
    "# Visualize GC content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b706d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC visualization is turned off.\n"
     ]
    }
   ],
   "source": [
    "if save_GC_plot:\n",
    "    save_dir = out_dir + \"./GC_visual_output\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "if visualize_GC:\n",
    "    for id, gc_content in chunks.items():\n",
    "        #format id\n",
    "        id = id.split(\"/\")[3]\n",
    "        \n",
    "        # create a bar plot\n",
    "        fig, ax = plt.subplots()\n",
    "        mean_gc = np.mean(gc_content)\n",
    "        colors = ['red' if abs(gc - mean_gc) / mean_gc * 100 >= threshold_pct else 'blue' for gc in gc_content]\n",
    "        plt.bar(range(len(gc_content)), gc_content, color=colors)\n",
    "        ax.axhline(y=gc_trimmed_mean[id], color='black')\n",
    "        plt.title(\"Contig: \" + str(id))\n",
    "        plt.xlabel('index of Chunk of size: ' + str(chunk_size))\n",
    "        plt.ylabel('GC content')\n",
    "\n",
    "        # save the plot\n",
    "        if save_GC_plot:\n",
    "            save_path = os.path.join(save_dir, \"GC_content_\" + str(id) + \".png\")\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"GC visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a8705",
   "metadata": {},
   "source": [
    "# Naive coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a58bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig(coverage_file):\n",
    "    naive_coverage = {}\n",
    "\n",
    "    with open(coverage_file, \"r\") as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            # Split line into columns\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "\n",
    "            #only add if length > min_contig_size\n",
    "            if (int(columns[2]) - int(columns[1])) > min_contig_size:\n",
    "                # Use the first column as key and the sixth column as value\n",
    "                key = columns[0].split(\"/\")[3]\n",
    "                value = columns[6]\n",
    "                length = int(columns[2]) - int(columns[1])\n",
    "\n",
    "                # Add key-value pair to dictionary\n",
    "                naive_coverage[key] = [float(value), length]\n",
    "            \n",
    "    f.close()\n",
    "    return naive_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cdd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "naive_coverage = get_coverage_per_contig(coverage_per_contig)\n",
    "print(len(naive_coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb8a1c",
   "metadata": {},
   "source": [
    "# Chunk averaged Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a896c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig_by_chunks(depth_file, chunk_size):\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    \n",
    "    # Define a dictionary to store the coverage data for each ID\n",
    "    id_data = {}\n",
    "\n",
    "    # Read in the TSV file\n",
    "    with open(depth_file, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "\n",
    "        # Iterate over each row in the file\n",
    "        for row in reader:\n",
    "            # Extract the ID, index, and coverage from the row\n",
    "            id = row[0].split(\"/\")[3]\n",
    "            index = int(row[1])\n",
    "            coverage = float(row[2])\n",
    "\n",
    "            # Check if this is the first row for this ID\n",
    "            if id not in id_data:\n",
    "                id_data[id] = []\n",
    "\n",
    "            # Find the window index for this row\n",
    "            window_index = index // chunk_size\n",
    "\n",
    "            # Check if there is already coverage data for this window\n",
    "            if len(id_data[id]) <= window_index:\n",
    "                # Add a new entry for this window\n",
    "                id_data[id].append({'window_sum': coverage, 'window_count': 1})\n",
    "            else:\n",
    "                # Add the coverage to the existing window data\n",
    "                id_data[id][window_index]['window_sum'] += coverage\n",
    "                id_data[id][window_index]['window_count'] += 1            \n",
    "                \n",
    "          \n",
    "    # Write the output to a file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", 'w') as outfile:\n",
    "        outfile.write(\"id\\tindex\\taverage_coverage\\n\")\n",
    "        for id, windows in id_data.items():\n",
    "            #check total length > min_contig_size\n",
    "            total_len = 0\n",
    "            for i, window in enumerate(windows):\n",
    "                total_len += window['window_count']\n",
    "                if total_len > min_contig_size:\n",
    "                    for i, window in enumerate(windows):\n",
    "                        if window['window_count'] > 0:\n",
    "                            avg_coverage = round((window['window_sum'] / window['window_count']), 2)\n",
    "                            outfile.write(f\"{id}\\t{i}\\t{avg_coverage}\\n\")\n",
    "                        \n",
    "        outfile.close()\n",
    "    \n",
    "    #now get the average coverage for each id\n",
    "    # Open the input file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", \"r\") as f:\n",
    "        next(f)\n",
    "        # Create a dictionary to store the total coverage and highest index for each id\n",
    "        id_coverage = {}\n",
    "        # Read through each line in the file\n",
    "        for line in f:\n",
    "            # Split the line into three columns\n",
    "            id, index, coverage = line.strip().split(\"\\t\")\n",
    "            # Convert the coverage to float\n",
    "            index = float(index)\n",
    "            coverage = float(coverage)\n",
    "            # If the id is not in the dictionary yet, add it with a coverage of 0 and index of -1\n",
    "            if id not in id_coverage:\n",
    "                id_coverage[id] = {\"coverage\": 0, \"max_index\": -1}\n",
    "            # Add the coverage to the total for this id\n",
    "            id_coverage[id][\"coverage\"] += coverage\n",
    "            # Update the max index for this id if the current index is higher\n",
    "            if index > id_coverage[id][\"max_index\"]:\n",
    "                id_coverage[id][\"max_index\"] = index\n",
    "    f.close()\n",
    "    \n",
    "    #make list of coverages, then take trimmed mean of that list\n",
    "    coverage_trimmed_mean = {}\n",
    "    for entry in id_data:\n",
    "        coverages = []\n",
    "        for chunk in id_data.get(entry):\n",
    "            window_sum = float(chunk.get('window_sum'))\n",
    "            window_count = float(chunk.get('window_count'))\n",
    "            coverage = round((window_sum / window_count), 2)\n",
    "            coverages.append(coverage)\n",
    "        \n",
    "        coverage_trimmed_mean[entry] = round(stats.trim_mean(coverages, 0.05), 2)\n",
    "        \n",
    "    # Open the output file\n",
    "    with open(save_dir + \"chunked_coverage.tsv\", \"w\") as f:\n",
    "        # Write the header row\n",
    "        f.write(\"id\\taverage_coverage\\n\")\n",
    "        # Loop through the ids in the dictionary\n",
    "        for id in coverage_trimmed_mean:\n",
    "            average_coverage = coverage_trimmed_mean[id]\n",
    "            f.write(\"{}\\t{}\\n\".format(id, average_coverage))\n",
    "    f.close()\n",
    "\n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb693cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/heliopora_coerulea_hifi_2_p_assembly.depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chunks_coverage \u001b[38;5;241m=\u001b[39m \u001b[43mget_coverage_per_contig_by_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoverage_per_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(chunks_coverage)\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mget_coverage_per_contig_by_chunks\u001b[1;34m(depth_file, chunk_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m id_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read in the TSV file\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tsvfile:\n\u001b[0;32m      9\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(tsvfile, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Iterate over each row in the file\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Heliopora\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/heliopora_coerulea_hifi_2_p_assembly.depth'"
     ]
    }
   ],
   "source": [
    "chunks_coverage = get_coverage_per_contig_by_chunks(coverage_per_base, chunk_size)\n",
    "len(chunks_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ae3a",
   "metadata": {},
   "source": [
    "# Visualize Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = out_dir + \"Coverage_output/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "#windows\n",
    "data = pd.read_csv(save_dir + \"chunked_windows_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "trimmed_mean = pd.read_csv(save_dir + \"chunked_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "\n",
    "# Convert data to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "mean = pd.DataFrame(trimmed_mean)\n",
    "# Group data by id\n",
    "groups = df.groupby('id')\n",
    "# Get a list of unique ids\n",
    "unique_ids = df['id'].unique()\n",
    "    \n",
    "if visualize_Coverage:\n",
    "    # Create a separate plot for each id\n",
    "    for id in unique_ids:\n",
    "        # Filter the dataframe to get only the rows for this id\n",
    "        df_id = df[df['id'] == id]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        trimmed_mean_coverage = float(mean.loc[mean['id'] == id, 'average_coverage'])\n",
    "        ax.axhline(y=trimmed_mean_coverage, color='black')\n",
    "\n",
    "        #filtered colors\n",
    "        colors = ['red' if abs(coverage - trimmed_mean_coverage) / trimmed_mean_coverage * 100 >= threshold_pct else 'blue' for coverage in df_id['average_coverage']]\n",
    "\n",
    "        # Plot the data for this id\n",
    "        plt.bar(df_id['index'], df_id['average_coverage'], label=f\"id={id}\", color=colors)\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel('Index of chunk size ' + str(chunk_size))\n",
    "        plt.ylabel('Average coverage')\n",
    "\n",
    "        # save the plot\n",
    "        if save_coverage_plot:\n",
    "            save_path = os.path.join(save_dir, \"Coverage_\" + str(id) + \".png\")\n",
    "            plt.savefig(save_path)\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "else :\n",
    "    print(\"Coverage visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d53fe",
   "metadata": {},
   "source": [
    "# Blob Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot gc content (x) vs coverage (y)\n",
    "if visualize_blob:\n",
    "    \n",
    "    #plot 1 naive\n",
    "    data = [{'id': k, 'gc_content': v} for k, v in naive_gc.items()]\n",
    "    data2 = [{'id': k, 'average_coverage': v[0], 'length': v[1]} for k, v in naive_coverage.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    naive_df = pd.merge(df1, df2, on = 'id')\n",
    "    naive_df = naive_df[naive_df['length'] >= plot_min_length]\n",
    "    print(naive_df)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim([0, 100])\n",
    "    cmap = plt.cm.get_cmap('plasma')\n",
    "    \n",
    "    sc = plt.scatter(naive_df['gc_content'], naive_df['average_coverage'], c = naive_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('GC content')\n",
    "    plt.ylabel('Average Coverage')\n",
    "    plt.title('Blob plot with naive averages')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    ax.legend([sc], ['Length'], loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    if save_blob_plot:\n",
    "        save_path = os.path.join(out_dir, \"Blob_naive.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beac461",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_blob:\n",
    "    #plot 2 trimmed/chunked\n",
    "    \n",
    "    data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "    trimmed_df = trimmed_df[trimmed_df['length'] >= plot_min_length]\n",
    "    print(trimmed_df) \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim([0, 100])\n",
    "    cmap = plt.cm.get_cmap('plasma')\n",
    "    \n",
    "    labels = list(trimmed_df['id'])\n",
    "    sc1 = plt.scatter(trimmed_df['gc_content'], trimmed_df['average_coverage'], c = trimmed_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc1)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('GC content')\n",
    "    plt.ylabel('Trimmed Average Coverage')\n",
    "    plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "    ax.legend([sc1], ['Length'], loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    if save_blob_plot:\n",
    "        save_path = os.path.join(out_dir, \"Blob.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e549a8",
   "metadata": {},
   "source": [
    "# Histogram of contig lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of all contig lengths\n",
    "\n",
    "if visualize_histogram:\n",
    "    #x = data = list of lengths of all contigs\n",
    "    #bins = size of bins\n",
    "    data = trimmed_df[\"length\"]\n",
    "    \n",
    "    plt.hist(data, histogram_bin_size)\n",
    "    \n",
    "    \n",
    "    plt.xlabel('Contig Lengths')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of contig lengths')\n",
    "    \n",
    "    # save the plot\n",
    "    if save_histogram_plot:\n",
    "        save_path = os.path.join(out_dir, \"Histogram_of_lengths.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c0786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594564f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cbac36be",
   "metadata": {},
   "source": [
    "#TODO \n",
    "1. add trim mean --done\n",
    "2. add mean to plot -- done\n",
    "3. highlight chunks outside a given range of mean -- done\n",
    "4. repeat 1-3 for coverage -- done\n",
    "5. blob plots --done\n",
    "6. make dataframe with id, gc, coverage for both naive and trimmed --done\n",
    "7. add option to ignore contigs under a certain length --done\n",
    "8. color blob contigs by length --done\n",
    "9. histogram of contig length --todo get from coverage file end-start, add to df\n",
    "10. make blob plots for N75-L75 N50-L50\n",
    "11. fasta file - for each contig -every 10k 1 k\n",
    "12. only take contigs until their sum length = genome size\n",
    "13. kmeans- spllit into clusters - keep ading contigs until sum length of cluster 1 is size of genome\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Heliopora",
   "language": "python",
   "name": "heliopora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
