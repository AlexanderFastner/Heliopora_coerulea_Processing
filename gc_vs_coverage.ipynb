{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b318748",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 get GC content from contigs\n",
    "    #get GC content in set chunk sizes\n",
    "    \n",
    "#2 get coverage for contigs\n",
    "    #get coverage in chunk sizes\n",
    "\n",
    "#3 blob plot gc vs average coverage\n",
    "\n",
    "#4 plot gc content/coverage for each contig (if significant in some way)\n",
    "\n",
    "#5 kmeans clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7684a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2893ee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ec1c42",
   "metadata": {},
   "source": [
    "assembly : the fasta input\n",
    "coverage_per_contig : A tsv file with information for each contig\n",
    "coverage_per_base:  A tsv file with coverage information for each base pair position\n",
    "out_dir : output directory\n",
    "min_contig_size : the minimum size of a contig, otherwise its ignored\n",
    "chunk_size : the chunk size for dividing up longer contigs (default 10000)\n",
    "visualize_GC : An option for creating plots showing the distribution of GC content accross a contig in steps of a given chunk size (default = False)\n",
    "save_GC_plot : option to create an output dir savinig a plot for each GC plot created by visualizeGC\n",
    "visualize_Coverage : An option to create plots showing coverage distribution per contig\n",
    "save_coverage_plot : option to save coverage plots in out dir\n",
    "threshold_pct : The percentage away from the mean that something needs to be in order to be highlighted\n",
    "visualize_blob : Option to visualize blob plot\n",
    "save_blob_plot : Option to save blob plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1180c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = \"./data/final.p_ctg.fasta\"\n",
    "coverage_per_contig = \"./data/heliopora_coerulea_hifi_2_p_assembly.coverage\"\n",
    "coverage_per_base = \"./data/heliopora_coerulea_hifi_2_p_assembly.depth\"\n",
    "out_dir = \"out_dir/\"\n",
    "min_contig_size = 25000\n",
    "chunk_size = 10000\n",
    "visualize_GC = True\n",
    "save_GC_plot = True\n",
    "visualize_Coverage = True\n",
    "save_coverage_plot = True\n",
    "threshold_pct = 50\n",
    "visualize_blob = True\n",
    "save_blob_plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b434043",
   "metadata": {},
   "source": [
    "# Naive GC content for each content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff775cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content(fasta_file):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to their GC content\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq y min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:\n",
    "                        gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "                        \n",
    "                # Start the new contig\n",
    "                current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "    \n",
    "    # Calculate GC content for the final contig\n",
    "    gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "    \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_gc = get_gc_content(assembly)\n",
    "#print(naive_gc)\n",
    "print(len(naive_gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad90d78",
   "metadata": {},
   "source": [
    "# GC content for each contig by averaging in given chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad03fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content_chunks(fasta_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to a list of gc content per chunk size\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq < min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:  \n",
    "                        i = 0\n",
    "                        while i < len(current_seq):\n",
    "                            # check for last chunk\n",
    "                            if (i + chunk_size) > len(current_seq):\n",
    "                                end = len(current_seq)\n",
    "                            else:\n",
    "                                end = i + chunk_size\n",
    "                            sub = current_seq[i:end]\n",
    "                            gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)),2)\n",
    "                            chunks.append(gc)\n",
    "                            i = end\n",
    "                        # add to dictionary in form {current_id : [array of GC content of given chunk size]}\n",
    "                        gc_content[current_id] = chunks\n",
    "\n",
    "                # Start the new contig\n",
    "                current_id = line.strip()[1:]\n",
    "                current_seq = \"\"\n",
    "                chunks = []\n",
    "                \n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "                \n",
    "        # calculate GC content for the last contig\n",
    "        if current_id is not None:\n",
    "            i = 0\n",
    "            while i < len(current_seq):\n",
    "                # check for last chunk\n",
    "                if (i + chunk_size) > len(current_seq):\n",
    "                    end = len(current_seq)\n",
    "                else:\n",
    "                    end = i + chunk_size\n",
    "                sub = current_seq[i:end]\n",
    "                gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)), 2)\n",
    "                chunks.append(gc)\n",
    "                i = end\n",
    "            # add to dictionary in form {current_id : [array of GC content of given chunk size]}\n",
    "            gc_content[current_id] = chunks\n",
    "        \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d18eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_gc_content_chunks(assembly, chunk_size)\n",
    "\n",
    "#print(chunks)\n",
    "gc_trimmed_mean = {}\n",
    "#Get the trimmed mean from chunks\n",
    "for entry in chunks:\n",
    "    #print(entry)\n",
    "    new_entry = entry.split(\"/\")[3]\n",
    "    gc_trimmed_mean[new_entry] = stats.trim_mean(chunks.get(entry), 0.05)\n",
    "print(len(gc_trimmed_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2afd",
   "metadata": {},
   "source": [
    "# Visualize GC content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b706d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_GC_plot:\n",
    "    save_dir = out_dir + \"./GC_visual_output\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "if visualize_GC:\n",
    "    for id, gc_content in chunks.items():\n",
    "        #format id\n",
    "        id = id.split(\"/\")[3]\n",
    "        \n",
    "        # create a bar plot\n",
    "        fig, ax = plt.subplots()\n",
    "        mean_gc = np.mean(gc_content)\n",
    "        colors = ['red' if abs(gc - mean_gc) / mean_gc * 100 >= threshold_pct else 'blue' for gc in gc_content]\n",
    "        plt.bar(range(len(gc_content)), gc_content, color=colors)\n",
    "        ax.axhline(y=gc_trimmed_mean[id], color='black')\n",
    "        plt.title(\"Contig: \" + str(id))\n",
    "        plt.xlabel('index of Chunk of size: ' + str(chunk_size))\n",
    "        plt.ylabel('GC content')\n",
    "\n",
    "        # save the plot\n",
    "        if save_GC_plot:\n",
    "            save_path = os.path.join(save_dir, \"GC_content_\" + str(id) + \".png\")\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"GC visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a8705",
   "metadata": {},
   "source": [
    "# Naive coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a58bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig(coverage_file):\n",
    "    naive_coverage = {}\n",
    "\n",
    "    with open(coverage_file, \"r\") as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            # Split line into columns\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "\n",
    "            #only add if length > min_contig_size\n",
    "            if (int(columns[2]) - int(columns[1])) > min_contig_size:\n",
    "                # Use the first column as key and the sixth column as value\n",
    "                key = columns[0].split(\"/\")[3]\n",
    "                value = columns[6]\n",
    "\n",
    "                # Add key-value pair to dictionary\n",
    "                naive_coverage[key] = float(value)\n",
    "            \n",
    "    f.close()\n",
    "    return naive_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_coverage = get_coverage_per_contig(coverage_per_contig)\n",
    "print(len(naive_coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb8a1c",
   "metadata": {},
   "source": [
    "# Chunk averaged Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a896c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig_by_chunks(depth_file, chunk_size):\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    \n",
    "    # Define a dictionary to store the coverage data for each ID\n",
    "    id_data = {}\n",
    "\n",
    "    # Read in the TSV file\n",
    "    with open(depth_file, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "\n",
    "        # Iterate over each row in the file\n",
    "        for row in reader:\n",
    "            # Extract the ID, index, and coverage from the row\n",
    "            id = row[0].split(\"/\")[3]\n",
    "            index = int(row[1])\n",
    "            coverage = float(row[2])\n",
    "\n",
    "            # Check if this is the first row for this ID\n",
    "            if id not in id_data:\n",
    "                id_data[id] = []\n",
    "\n",
    "            # Find the window index for this row\n",
    "            window_index = index // chunk_size\n",
    "\n",
    "            # Check if there is already coverage data for this window\n",
    "            if len(id_data[id]) <= window_index:\n",
    "                # Add a new entry for this window\n",
    "                id_data[id].append({'window_sum': coverage, 'window_count': 1})\n",
    "            else:\n",
    "                # Add the coverage to the existing window data\n",
    "                id_data[id][window_index]['window_sum'] += coverage\n",
    "                id_data[id][window_index]['window_count'] += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "    for id, windows in id_data.items():\n",
    "        total_len = 0\n",
    "        for i, window in enumerate(windows):\n",
    "            total_len += window['window_count']\n",
    "        if total_len < min_contig_size:            \n",
    "                \n",
    "          \n",
    "    # Write the output to a file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", 'w') as outfile:\n",
    "        outfile.write(\"id\\tindex\\taverage_coverage\\n\")\n",
    "        for id, windows in id_data.items():\n",
    "            #check total length > min_contig_size\n",
    "            total_len = 0\n",
    "            for i, window in enumerate(windows):\n",
    "                total_len += window['window_count']\n",
    "                if total_len > min_contig_size:\n",
    "                    for i, window in enumerate(windows):\n",
    "                        if window['window_count'] > 0:\n",
    "                            avg_coverage = round((window['window_sum'] / window['window_count']), 2)\n",
    "                            outfile.write(f\"{id}\\t{i}\\t{avg_coverage}\\n\")\n",
    "                        \n",
    "        outfile.close()\n",
    "    \n",
    "    #now get the average coverage for each id\n",
    "    # Open the input file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", \"r\") as f:\n",
    "        next(f)\n",
    "        # Create a dictionary to store the total coverage and highest index for each id\n",
    "        id_coverage = {}\n",
    "        # Read through each line in the file\n",
    "        for line in f:\n",
    "            # Split the line into three columns\n",
    "            id, index, coverage = line.strip().split(\"\\t\")\n",
    "            # Convert the coverage to float\n",
    "            index = float(index)\n",
    "            coverage = float(coverage)\n",
    "            # If the id is not in the dictionary yet, add it with a coverage of 0 and index of -1\n",
    "            if id not in id_coverage:\n",
    "                id_coverage[id] = {\"coverage\": 0, \"max_index\": -1}\n",
    "            # Add the coverage to the total for this id\n",
    "            id_coverage[id][\"coverage\"] += coverage\n",
    "            # Update the max index for this id if the current index is higher\n",
    "            if index > id_coverage[id][\"max_index\"]:\n",
    "                id_coverage[id][\"max_index\"] = index\n",
    "    f.close()\n",
    "    \n",
    "    #make list of coverages, then take trimmed mean of that list\n",
    "    coverage_trimmed_mean = {}\n",
    "    for entry in id_data:\n",
    "        coverages = []\n",
    "        for chunk in id_data.get(entry):\n",
    "            window_sum = float(chunk.get('window_sum'))\n",
    "            window_count = float(chunk.get('window_count'))\n",
    "            coverage = round((window_sum / window_count), 2)\n",
    "            coverages.append(coverage)\n",
    "        \n",
    "        coverage_trimmed_mean[entry] = round(stats.trim_mean(coverages, 0.05), 2)\n",
    "        \n",
    "    # Open the output file\n",
    "    with open(save_dir + \"chunked_coverage.tsv\", \"w\") as f:\n",
    "        # Write the header row\n",
    "        f.write(\"id\\taverage_coverage\\n\")\n",
    "        # Loop through the ids in the dictionary\n",
    "        for id in coverage_trimmed_mean:\n",
    "            average_coverage = coverage_trimmed_mean[id]\n",
    "            f.write(\"{}\\t{}\\n\".format(id, average_coverage))\n",
    "    f.close()\n",
    "\n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb693cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_coverage = get_coverage_per_contig_by_chunks(coverage_per_base, chunk_size)\n",
    "len(chunks_coverage)\n",
    "\n",
    "#new change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ae3a",
   "metadata": {},
   "source": [
    "# Visualize Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_coverage_plot:\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "if visualize_Coverage:\n",
    "    \n",
    "    #windows\n",
    "    data = pd.read_csv(save_dir + \"chunked_windows_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "    trimmed_mean = pd.read_csv(save_dir + \"chunked_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "    \n",
    "    # Convert data to pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    mean = pd.DataFrame(trimmed_mean)\n",
    "    # Group data by id\n",
    "    groups = df.groupby('id')\n",
    "    # Get a list of unique ids\n",
    "    unique_ids = df['id'].unique()\n",
    "    \n",
    "    # Create a separate plot for each id\n",
    "    for id in unique_ids:\n",
    "        # Filter the dataframe to get only the rows for this id\n",
    "        df_id = df[df['id'] == id]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        trimmed_mean_coverage = float(mean.loc[mean['id'] == id, 'average_coverage'])\n",
    "        ax.axhline(y=trimmed_mean_coverage, color='black')\n",
    "\n",
    "        #filtered colors\n",
    "        colors = ['red' if abs(coverage - trimmed_mean_coverage) / trimmed_mean_coverage * 100 >= threshold_pct else 'blue' for coverage in df_id['average_coverage']]\n",
    "\n",
    "        # Plot the data for this id\n",
    "        plt.bar(df_id['index'], df_id['average_coverage'], label=f\"id={id}\", color=colors)\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel('Index of chunk size ' + str(chunk_size))\n",
    "        plt.ylabel('Average coverage')\n",
    "\n",
    "        # save the plot\n",
    "        if save_coverage_plot:\n",
    "            save_path = os.path.join(save_dir, \"Coverage_\" + str(id) + \".png\")\n",
    "            plt.savefig(save_path)\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "else :\n",
    "    print(\"Coverage visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d53fe",
   "metadata": {},
   "source": [
    "# Blob Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot gc content (x) vs coverage (y)\n",
    "if visualize_blob:\n",
    "    \n",
    "    #plot 1 naive\n",
    "    data = [{'id': k, 'gc_content': v} for k, v in naive_gc.items()]\n",
    "    data2 = [{'id': k, 'average_coverage': v} for k, v in naive_coverage.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    naive_df = pd.merge(df1, df2, on = 'id')\n",
    "    print(naive_df)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_ylim([0, 100])\n",
    "    \n",
    "    plt.scatter(naive_df['gc_content'], naive_df['average_coverage'])\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('GC content')\n",
    "    plt.ylabel('Average Coverage')\n",
    "    plt.title('Blob plot with naive averages')\n",
    "    #plt.legend(list(naive_df['id']), loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    if save_blob_plot:\n",
    "        save_path = os.path.join(out_dir, \"Blob_naive.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beac461",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_blob:\n",
    "    #plot 2 trimmed/chunked\n",
    "    \n",
    "    data = [{'id': k, 'gc_content': v} for k, v in gc_trimmed_mean.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "    #print(trimmed_df) \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim([0, 100])\n",
    "    \n",
    "    labels = list(trimmed_df['id'])\n",
    "    plt.scatter(trimmed_df['gc_content'], trimmed_df['average_coverage'])\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('GC content')\n",
    "    plt.ylabel('Trimmed Average Coverage')\n",
    "    plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "    #plt.legend(labels, loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    if save_blob_plot:\n",
    "        save_path = os.path.join(out_dir, \"Blob.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59264dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trimmed_df[trimmed_df['average_coverage'] < 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388c024",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbac36be",
   "metadata": {},
   "source": [
    "#TODO \n",
    "1. add trim mean --done\n",
    "2. add mean to plot -- done\n",
    "3. highlight chunks outside a given range of mean -- done\n",
    "4. repeat 1-3 for coverage -- done\n",
    "5. blob plots --done\n",
    "6. make dataframe with id, gc, coverage for both naive and trimmed --done\n",
    "7. add option to ignore contigs under a certain length --todo\n",
    "8. color blob contigs by length --todo\n",
    "9. histogram of contig length --todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
